<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title></title>
    <link href="https://noamf.ink/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://noamf.ink"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2023-03-20T00:00:00+00:00</updated>
    <id>https://noamf.ink/atom.xml</id>
    <entry xml:lang="en">
        <title>Types of curiosity: predictive and anomalous</title>
        <published>2023-03-20T00:00:00+00:00</published>
        <updated>2023-03-20T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://noamf.ink/posts/predictive-and-anomalous-curiosity/" type="text/html"/>
        <id>https://noamf.ink/posts/predictive-and-anomalous-curiosity/</id>
        <content type="html">&lt;p&gt;A couple days ago I wrote about an idea for &lt;a href=&quot;https:&#x2F;&#x2F;noamf.ink&#x2F;posts&#x2F;intrinsic-reinforcement-learning&#x2F;&quot;&gt;curiosity in reinforcement
learning&lt;&#x2F;a&gt;. Since then I was reminded that
there&#x27;s already a class of methods in reinforcement learning that go by the same name, summarized in
&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1808.04355.pdf&quot;&gt;this paper&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;These methods use a different idea to generate &amp;quot;curiosity.&amp;quot; They allow an agent to make predictions
about the effects of its actions. The more incorrect the agent is about its predictions, the more
it&#x27;s penalized. It has an incentive to learn more about the kinds of situations that leave it least
about to predict the future.&lt;&#x2F;p&gt;
&lt;p&gt;The kind of curiosity I was writing about is different. The idea there would be to simulate the
agent&#x27;s &amp;quot;mental model&amp;quot; of the world by using a model trained by an unsupervised approach. The agent
might contain, in part, an auto-encoder element that encodes the full state of the environment or
distinct objects in the environment. When the agent comes across input that the auto-encoder encodes
poorly, it gets a poor reward, indicating that input is &lt;em&gt;anomalous&lt;&#x2F;em&gt; in what the agent has seen so
far.&lt;&#x2F;p&gt;
&lt;p&gt;I think human curiosity involves elements of both of these paradigms (and probably many others).
It&#x27;s true that we&#x27;re constantly making predictions about what we think will happen as a result of
our actions, and that when those predictions are wrong, that&#x27;s an opportunity for learning. We can
get curious about why we were wrong, and try to figure out how to be less wrong in the future.&lt;&#x2F;p&gt;
&lt;p&gt;But curiosity doesn&#x27;t require this kind of prediction. When we encounter something that&#x27;s totally
new to us — something we don&#x27;t quite know how to make sense of — we can get curious
about it, even without making predictions. This kind of curiosity is better simulated by an
unsupervised loss, which is evaluated on how well its &amp;quot;mental model&amp;quot; accords with reality. Just as
we&#x27;re driven to find out more about &lt;em&gt;anomalies&lt;&#x2F;em&gt; in our lives, the agent would be driven to refine
its &amp;quot;mental model&amp;quot; to account for the input that strikes it as strange. I think of this as a more
&lt;em&gt;reflective&lt;&#x2F;em&gt; kind of curiosity. As we try to synthesize and assimilate information about the world,
we notice and wonder about pieces that don&#x27;t quite fit.&lt;&#x2F;p&gt;
&lt;p&gt;What would happen if we created an agent with both predictive and anomalous curiosity? I wonder.&lt;&#x2F;p&gt;
</content>
    </entry>
    <entry xml:lang="en">
        <title>The Google maps effect: A reason to be cautious about using AI coding assistants</title>
        <published>2023-03-19T00:00:00+00:00</published>
        <updated>2023-03-19T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://noamf.ink/posts/google-maps-effect/" type="text/html"/>
        <id>https://noamf.ink/posts/google-maps-effect/</id>
        <content type="html">&lt;p&gt;There are plenty of upsides to using Google maps. The first few times you go somewhere new, it&#x27;s
easier to get there. You don&#x27;t have to look up directions, you&#x27;re not going to miss a turn. The
route you take may have less traffic on it. You might even get advance warning about a speed trap.&lt;&#x2F;p&gt;
&lt;p&gt;But there are downsides too. By handing over control of your route to the machine, you become a sort
of direction-following machine yourself. You don&#x27;t have to look for landmarks along the way, or even
to register which streets you&#x27;re driving on. As long as you &lt;em&gt;turn left at the light&lt;&#x2F;em&gt; when told,
you&#x27;ll get where you&#x27;re going.&lt;&#x2F;p&gt;
&lt;p&gt;As a result, a common phenomenon is that people who are regular users of things like Google maps
never develop a familiarity with there physical landscapes. They don&#x27;t understand the infrastructure
well enough to reason about how to adjust their trip if necessary, or about why their might be
traffic on certain roads at specific times. It&#x27;s not uncommon for people to use Google maps on trips
they&#x27;ve taken dozens of times.&lt;&#x2F;p&gt;
&lt;p&gt;I think of this as the Google maps effect. We get enticed into handing off responsibility to a
machine by short term, substantial benefits. But as a result we become reliant on the machine. We
become strangers on our own roads. Worse still, we can lose access to a certain kind of spatial &#x2F;
transportational reasoning capacity that was genuinely valuable, and genuinely rewarding to use. We
can lose the &lt;em&gt;sensation&lt;&#x2F;em&gt; of where we are, in the context of where we&#x27;ve been and where we&#x27;re going.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s easy to imagine that using AI coding assistants will lead to a similar result, albeit in
developing software rather than in navigating the physical world. The most recent models are able to
load entire repositories as context for answering specific questions, or generating code for
specific tasks. A developer using these AI tools doesn&#x27;t need nearly as strong of mental model for
how the code is structured. And because the AI tools can &amp;quot;write&amp;quot; code snippets, the developer
doesn&#x27;t need nearly as strong of an understanding of the conceptual layout of the libraries they&#x27;re
using either. Over time, I suspect developers using these tools will get more and more out of touch
with their own code bases, with the libraries they&#x27;re using to develop, and eventually, with the
code they write themselves (with the help of the AI). As with Google maps, we&#x27;ll lose an important
dimension to our reasoning.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s a lot of excitement right now about what an AI code assistant can do. People ask it to
generate code for certain tasks, in non-trivial contexts, and it can provide answers that are either
correct or nearly correct. It looks like it can save it quite a lot of effort. We don&#x27;t have to
learn all about some relevant libraries, we don&#x27;t have to invest a lot of time into understanding
how all the pieces of the code we&#x27;re dealing with fit together, and so on. But the side effect of
investing all that time is that we learn, and the things that we learn provide context for us to
create better and better solutions over time. What happens when all that time investment — and
all that learning — become optional?&lt;&#x2F;p&gt;
&lt;p&gt;I think the current wave of excitement is similar to what we saw when Google maps first came out,
and it became possible to take complex journeys without getting detailed directions or pouring over
maps. But it&#x27;s important to think about the long-term. The tools we use change the work we do, and
over time, they change us too. We have to think carefully about just how they&#x27;re changing us, and
make sure it&#x27;s in a way we&#x27;re comfortable with. Ideally, it&#x27;ll be in a way that&#x27;s not just improving
short-term efficiency, but improving us as well.&lt;&#x2F;p&gt;
&lt;p&gt;Even if we only care about productivity, and not about the effect on ourselves as reasoning
entities, it&#x27;s not totally clear that AI code assistants as they currently stand will help with
long-term efficiency. A trip with Google maps is fleeting. Code isn&#x27;t. My suspicion is that as we
produce more code that we don&#x27;t understand well enough to have written ourselves, it will get harder
for us to reason about the code and add to it productively.&lt;&#x2F;p&gt;
&lt;p&gt;There are plenty of other reasons to be wary of AI assistants. They can produce plausible seeming
results that have subtle bugs, which means we &lt;em&gt;should&lt;&#x2F;em&gt; regard all their output very critically.
Debugging is harder than writing code. If we&#x27;re properly evaluating and testing the outputs of these
models for bugs, have we saved ourselves any time? And if we&#x27;re not, what kinds of problems might we
be causing ourselves and the users of our code? And so on.&lt;&#x2F;p&gt;
&lt;p&gt;We may eventually get to a set of AI tools that make us more efficient, both short term and long
term, without hurting our own abilities. Proper coding assistants will take time, effort and thought
to develop. I suspect that using the current generation of models — trained on a different
task, without consideration of this use — will hurt our relationship to our work, in the same
way Google maps has shallowed our relationship to navigating and understanding our surroundings.
Until these tools get better, it&#x27;s worth being cautious.&lt;&#x2F;p&gt;
</content>
    </entry>
    <entry xml:lang="en">
        <title>Generating curiosity: Intrinsic rewards in reinforcement learning</title>
        <published>2023-03-18T00:00:00+00:00</published>
        <updated>2023-03-18T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://noamf.ink/posts/intrinsic-reinforcement-learning/" type="text/html"/>
        <id>https://noamf.ink/posts/intrinsic-reinforcement-learning/</id>
        <content type="html">&lt;p&gt;In the reinforcement learning paradigm, an agent exists in an environment, and can take actions that
alter the environment&#x27;s state. Every so often, the agent gets feedback about whether its actions
have made its situation better or worse, as evaluated by the so-called &lt;em&gt;reward function&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In some cases, the reward function looks &lt;em&gt;only&lt;&#x2F;em&gt; at the environment. For example, a reinforcement
learning approach to chess might give the agent a positive reward if it wins the game, or
incremental positive rewards if it makes good moves. Nothing about the reward depends on the agent&#x27;s
internal state.&lt;&#x2F;p&gt;
&lt;p&gt;In other cases, the reward function looks only at the agent&#x27;s state. A reinforcement learning
benchmark called &lt;a href=&quot;https:&#x2F;&#x2F;generallyintelligent.com&#x2F;avalon&#x2F;&quot;&gt;Avalon&lt;&#x2F;a&gt; takes this approach. In this
case, the agent exists in an adversarial environment and has to survive by eating. In essence, the
agent gets a positive reward whenever it eats. The state of the environment itself is irrelevant.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s sometimes argued that reinforcement learning is more reflective of how people learn than
supervised or unsupervised learning. &lt;a href=&quot;https:&#x2F;&#x2F;generallyintelligent.com&#x2F;&quot;&gt;Generally Intelligent&lt;&#x2F;a&gt;, the
company behind Avalon, have a tagline that reflects this idea:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;We want machines that learn the way humans do.&amp;quot; – Generally Intelligent&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;People do learn through something like reinforcement learning, which we usually call trial and
error. We also learn through something like supervised learning, though there the analogy isn&#x27;t as
good. Even in cases when we&#x27;re being explicitly taught, we&#x27;re usually taught with reference to
&lt;em&gt;general&lt;&#x2F;em&gt; rules and principles, rather than only being fed examples. In other words, the
&amp;quot;supervision&amp;quot; for humans isn&#x27;t just labels for the data, as it is for machines. It&#x27;s &lt;em&gt;also&lt;&#x2F;em&gt; a
conceptual framework for how to derive labels or perform an action. I don&#x27;t think we have great
tools yet for using a similar approach with machines, but we&#x27;ll need them if we want machines to
learn like people.&lt;&#x2F;p&gt;
&lt;p&gt;In this post, though, I&#x27;m more interested in the role of unsupervised learning in how people learn,
and in how that might be reflected in a reinforcement learning environment. There are important,
rewarding things people can do, that seem to alter neither the environment nor their internal state
in any obvious ways. There are things like meditating, going for a walk to think after a long day,
sitting outside and watching the trees sway in the wind. After we eat, there&#x27;s an objective
physiological change, mimicked by the reward function in Avalon. But what happens after we think?&lt;&#x2F;p&gt;
&lt;p&gt;One way to think about unsupervised learning is as the process of searching for patterns in the
data. It&#x27;s a process of synthesizing, interpreting, and assimilating information. In my view,
there&#x27;s a strong analogy to be made between that process and the process of thinking, or of sleeping
for that matter. There&#x27;s plenty of evidence that the more we pay attention to something — i.e.
the more thought we devote to it — the more firmly it will establish itself in our minds.
There&#x27;s also evidence that good sleep is essential to the formation of long-term memories. And,
I&#x27;d argue, the process of turning things over in your mind until you find a better way to
understand them is both rewarding and an essential part of learning.&lt;&#x2F;p&gt;
&lt;p&gt;As agents move through their environments, they collect information. It&#x27;s conceivable that there&#x27;s
order in that information that they may not pick up on when they&#x27;re purely trying to optimize for an
extrinsic — or in the case of Avalon, &lt;em&gt;objective&lt;&#x2F;em&gt; intrinsic — reward.&lt;&#x2F;p&gt;
&lt;p&gt;What if part of the agent&#x27;s reward was a more &lt;em&gt;subjective&lt;&#x2F;em&gt; intrinsic reward? In particular, what if
the agent was rewarded for finding compelling and explanatory patterns in the data? Assuming that
computing resources are not an issue, artificial agents wouldn&#x27;t need to take time out to think, as
we do. They can multi-task. So they can do unsupervised learning on data as a background process,
and potentially use the results to help decide on actions.&lt;&#x2F;p&gt;
&lt;p&gt;Rewarding the agent for the quality of the patterns it finds its environment will, hopefully, induce
it to find more compelling patterns. But there&#x27;s a more interesting effect of this kind of intrinsic
reward as well. Suppose the agent has developed an unsupervised representation of its environment
that aligns well with &lt;em&gt;most&lt;&#x2F;em&gt; of the data. But there&#x27;s a small set of data that, according to
whatever metric, is not well-represented. That small set of data will drive down the reward for the
quality of the agent&#x27;s unsupervised learning. The agent will have an incentive to explore its
environment and action spaces in such a way that it acquires the kind of information that lets it
make sense of the troubling data.&lt;&#x2F;p&gt;
&lt;p&gt;To me, this looks and functions a lot like curiosity in people. We get curious when we come across
something that doesn&#x27;t fit our mental model of the world, or that&#x27;s simply so far outside of our
realm of experience that we don&#x27;t really know what to make of it. That curiosity drives us to try
new things and see what happens, or to learn more about what&#x27;s around us.&lt;&#x2F;p&gt;
&lt;p&gt;There will undoubtedly be technical challenges to integrate this kind of unsupervised learning
approach to the reinforcement learning paradigm, but I think it&#x27;ll be worth trying. Curiosity is
arguably the main driver of human learning; if the goal is to have machines that learn like we do,
we&#x27;ll need some way to make them curious.&lt;&#x2F;p&gt;
&lt;p&gt;Of course, the process of learning — the process of satisfying curiosity — is rewarding
in its own right as well. That might be a topic for another time.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;noamf.ink&#x2F;posts&#x2F;predictive-and-anomalous-curiosity&#x2F;&quot;&gt;Addendum&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
    </entry>
    <entry xml:lang="en">
        <title>Thoughts on raising intelligent software</title>
        <published>2023-03-17T00:00:00+00:00</published>
        <updated>2023-03-17T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://noamf.ink/posts/raising-software/" type="text/html"/>
        <id>https://noamf.ink/posts/raising-software/</id>
        <content type="html">&lt;p&gt;Ted Chiang&#x27;s story &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Lifecycle_of_Software_Objects&quot;&gt;&lt;em&gt;The Lifecycle of Software
Objects&lt;&#x2F;em&gt;&lt;&#x2F;a&gt; is a literary exploration
of what might happen if artificially intelligent beings were raised, instead of &amp;quot;trained&amp;quot;. In the
story, digital entities are given a virtual environment to explore, and interact with humans and
with each other. Over the years, they develop similarly to how people or animals develop over time.&lt;&#x2F;p&gt;
&lt;p&gt;The dominant paradigm for training artificial intelligences is, of course, based on a very different
idea. Even the terminology is different, in important ways. When we talk about &lt;em&gt;natural&lt;&#x2F;em&gt;
intelligences, i.e. about people or about animals, &lt;em&gt;training&lt;&#x2F;em&gt; has to with attaining a specific
ability. But we would never say that that general intelligence — or sentience, or
consciousness — is a result of training. When it comes to artificial intelligence, though,
some prominent voices in the field believe that we can generate general intelligence through
training. Training for machine learning is not that different from training for people, or for
animals. In all cases, the trainee is basically asked to do the same things over and over, improving
from feedback on each attempt, until he&#x2F;she&#x2F;they&#x2F;it is competent.&lt;&#x2F;p&gt;
&lt;p&gt;A lot of people have pointed out that it&#x27;s a bit strange to think that general intelligence will
arise from training a model on a handful, or even a few score of tasks. In the cases of recent
large-scale models, those tasks are things like predicting the most likely next word, given context,
or predicting a textual description of an image. The training data is important to the training
process, but so is the task itself. Given the same training data, it&#x27;s possible to train models on
very different tasks, with very different outcomes.&lt;&#x2F;p&gt;
&lt;p&gt;Advocates for training-based approaches think that there&#x27;s enough information about the process of
&lt;em&gt;thinking&lt;&#x2F;em&gt; in the data, specifically in the context of the relevant tasks, that in mastering the
tasks, an artificial intelligence can also learn to &amp;quot;think&amp;quot;. Skeptics of this idea point out that
there are kinds of reasoning fundamental to intelligence that &lt;em&gt;cannot&lt;&#x2F;em&gt; be captured by this kind of
training. Two examples are &lt;em&gt;causal&lt;&#x2F;em&gt; reasoning — leading thinkers on causality sometimes
disparagingly call this kind of machine learning &amp;quot;curve fitting&amp;quot; — and symbolic reasoning.&lt;&#x2F;p&gt;
&lt;p&gt;I think these skeptics are right, and that it will be important to find ways to integrate these
kinds of reasoning. But I also think that to some degree they&#x27;re missing the point. Integrating
causal or symbolic reasoning into the training process will just expand the number of skills the
artificial intelligence is capable of learning. I don&#x27;t know of any methods in these fields that
would allow a model to develop its own symbolic taxonomies, or its own strategies for learning
causal relationships. My belief is that it will be difficult to develop such methods without
reorienting our thinking about what it means to develop artificial intelligence.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m curious about what it would mean to approach artificial intelligence from the perspective of
&amp;quot;raising&amp;quot; a model, instead of &amp;quot;training&amp;quot; it. What kinds of models are amenable to being &amp;quot;raised&amp;quot;?
What kind of simulated environment can we use to raise them? How much of it can we automate? As
implied by Ted Chiang&#x27;s story (and actually, by &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dacey%27s_Patent_Automatic_Nanny&quot;&gt;another of his
stories&lt;&#x2F;a&gt; as well), I suspect it&#x27;s
less than we might think.&lt;&#x2F;p&gt;
</content>
    </entry>
</feed>
