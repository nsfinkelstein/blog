<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Noam Finkelstein</title>
<link>https://noamf.ink/blog/</link>
<atom:link href="https://noamf.ink/blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Thu, 15 Feb 2024 05:00:00 GMT</lastBuildDate>
<item>
  <title>Trying out Table Parsing Models on Labwork Reports</title>
  <link>https://noamf.ink/blog/posts/pdf-lab-reports/</link>
  <description><![CDATA[ 





<p>This post is part of a <a href="../../../blog/posts/pdf-series-intro">series</a> about Document AI, specifically with an eye towards parsing medical documents. In this post, we’ll take a quick look at how a few <a href="../../../blog/posts/pdf-parsing-tasks#table-parsing">table parsing</a> tools do when applied to lab test reports.</p>
<section id="structure-of-lab-reports" class="level3">
<h3 class="anchored" data-anchor-id="structure-of-lab-reports">Structure of lab reports</h3>
<p>Below are three sample reports from three different labs.</p>
<p><a href="report-images/accesa-000.png" class="lightbox" data-gallery="unmarked"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/report-images/accesa-000.png" class="img-fluid" width="200"></a> <a href="report-images/labcorp-002.png" class="lightbox" data-gallery="unmarked"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/report-images/labcorp-002.png" class="img-fluid" width="200"></a> <a href="report-images/quest-000.png" class="lightbox" data-gallery="unmarked"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/report-images/quest-000.png" class="img-fluid" width="200"></a></p>
<p>The reports are basically tables, but they have two interesting features that will make them difficult to parse.</p>
<p>First, the tables sometimes include sub-headers, which specify the name of the lab panel that the test falls under. This is in evidence in the third report, but it’s a bit of an unusual case because each panel has only one test. In some cases, some but not all tests are associated with panels.</p>
<p>Second, the tables contain comments. In the second document, the comments actually sometimes contain the result of the lab test. In the third document, the comments apply to the lab test procedure itself, not to the result for an individual patient. In both cases, the comments warp the structure of the table.</p>
<p>As far as I know, there isn’t a great open dataset of lab reports, primarily because the include private health information. That said, a number of people have uploaded their own reports to the internet, and I used many of them in these checks. I didn’t include them in the post due to privacy considerations. Running table parsing models on those reports didn’t reveal anything unexpected, but they did show that the models have a much harder time with noisy, scanned documents.</p>
</section>
<section id="table-transformer-model" class="level3">
<h3 class="anchored" data-anchor-id="table-transformer-model">Table Transformer Model</h3>
<p>The open-source <a href="https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer">Table Transformer</a> is trained on detecting tables and identifying their structure. The model and training data are described in <a href="https://arxiv.org/abs/2110.00061">this paper</a>. The same base model is trained on two separate tasks, described below.</p>
<section id="table-detection" class="level4">
<h4 class="anchored" data-anchor-id="table-detection">Table detection</h4>
<p>The first task is to identify tables in a document.</p>
<p><a href="tatr-report-images/accesa-000.png" class="lightbox" data-gallery="tatr"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/tatr-report-images/accesa-000.png" class="img-fluid" width="200"></a> <a href="tatr-report-images/labcorp-002.png" class="lightbox" data-gallery="tatr"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/tatr-report-images/labcorp-002.png" class="img-fluid" width="200"></a> <a href="tatr-report-images/quest-000.png" class="lightbox" data-gallery="tatr"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/tatr-report-images/quest-000.png" class="img-fluid" width="200"></a></p>
<p>In two cases, the model has a hard time distinguishing between structured information at the top of the document, and the structured lab results. In the second document, the model seems to be tricked by the space in the table introduced by the longest comment. It may also be relevant that the records below that table have more spacing between them than those above, as they each have their own (short) comment. It’s also interesting that the model has high confidence for the third document, but relatively low confidence for the first two.</p>
</section>
<section id="table-structure-identification" class="level4">
<h4 class="anchored" data-anchor-id="table-structure-identification">Table structure identification</h4>
<p>To run the structure identification model, we first pull the highest-confidence table identified by the detection model above. The structure identification model is very sensitive to the amount of padding around the table it receives as input. This is a noted issue with the pre-trained model, and can likely be resolved by fine-tuning with an augmented dataset. For the purposes of using the pre-trained model, I had to add padding around the tables identified above to get reasonable predictions. It’s possible that adding more or less padding would result in better predictions, but because that would be a superficial solution to the problem, I didn’t take the time to find out.</p>
<p>Running this model produces the following results.</p>
<p><a href="tatr-structure-report-images/accesa-000.png" class="lightbox" data-gallery="tatr-segmented"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/tatr-structure-report-images/accesa-000.png" class="img-fluid" width="200"></a> <a href="tatr-structure-report-images/labcorp-002.png" class="lightbox" data-gallery="tatr-segmented"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/tatr-structure-report-images/labcorp-002.png" class="img-fluid" width="200"></a> <a href="tatr-structure-report-images/quest-000.png" class="lightbox" data-gallery="tatr-segmented"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/tatr-structure-report-images/quest-000.png" class="img-fluid" width="200"></a></p>
<p>The model does a relatively good job with the first document, though not as good as I would have expected given how structured the table is. In the second document, it seems to be exactly correct for the part of the table identified by the table detection model, but has no change on the rest of the table (I did not separately extract the rest of the table to see how it would have done). In the third document, the model doesn’t recognize the rows corresponding to the antibody tests. This might have something to do with the fact the test names extend past what it has identified as the relevant column. The size of the column seems to have to do with the front-matter, rather than the lab tests.</p>
</section>
</section>
<section id="commercial-options" class="level3">
<h3 class="anchored" data-anchor-id="commercial-options">Commercial options</h3>
<p>There are a few commercial options for table parsing. The easiest to try out was Nanonets, which produced the outputs below.</p>
<p><a href="nanonets-report-images/accesa-000.png" class="lightbox" data-gallery="nanonets"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/nanonets-report-images/accesa-000.png" class="img-fluid" width="200"></a> <a href="nanonets-report-images/labcorp-002.png" class="lightbox" data-gallery="nanonets"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/nanonets-report-images/labcorp-002.png" class="img-fluid" width="200"></a> <a href="nanonets-report-images/quest-000.png" class="lightbox" data-gallery="nanonets"><img src="https://noamf.ink/blog/posts/pdf-lab-reports/nanonets-report-images/quest-000.png" class="img-fluid" width="200"></a></p>
<p>The first striking part of this result is that none of the table cells overlap. I don’t know if this is due to post-processing or because of how they trained their model, but it’s nice. Their structure detection seems to be better, by and large, but also less flexible, as there is no notion of a spanning cell or a header row.</p>
<p>The first document is pretty much exactly right. In the second document, they miss the last lab test, and also separate the <code>REFERENCE INTERVAL</code> column into two columns, which would cause problems downstream. In the third document, they also miss the last lab, but otherwise do well. Because they’re missing a concept for a spanning cell, in the last two documents the comments are being sliced up. This might be addressable in post-processing.</p>
</section>
<section id="next-steps" class="level3">
<h3 class="anchored" data-anchor-id="next-steps">Next steps</h3>
<p>The two models both had difficulty with the kinds of things I expected they might have difficulty with, discussed above. Additionally, the open source model had difficulty distinguishing between the lab test results and structured text at the top of the document, which I did not anticipate.</p>
<p>Any training or fine-tuning to get around these kinds of problems would have to be based on synthetic data, because there is not enough open lab-report data out there.</p>
<p>Here are a few considerations for generating this kind of data set:</p>
<ul>
<li>Reports should have structured text at the top that looks like it <em>might</em> be part of the lab result table but is not, to help the model distinguish between form fields and lab results.</li>
<li>The lab results table should include header row columns for lab tests, with representation of the cases in which all, some, or no labs have associated panels, and a good distribution over the number of tests per panel.</li>
<li>The lab results rows should sometimes span multiple lines, with overflow wrapping at both the column and row level (this caused issues in lab reports not shown in this post).</li>
<li>The lab results table should have both inline and stand-alone comments.</li>
<li>Ideally the lab names, reference ranges, values, units, etc. used in the generated document should all be plausible. Depending on how the model is trained, this might help with text recognition.</li>
</ul>
<p>I don’t think it would be difficult to create a dataset with these properties that also contains enough variety to train for every lab report I’ve come across. Because there are lots of little labs out there, and their reports all have different formats, it would be prudent to add even more structural variety than that, even though that will likely increase the required model size and the training burden.</p>
<p>One other thought is that the approach of these table parsing models seems somewhat indirect. They first visually identify the table and its structure, and then run OCR on each cell. I wonder whether there is a more direct approach, where the input is the lab report document, and the output is one or more CSV files containing the results.</p>
<p>Overall, I don’t think general purpose table parsing tools will be effective for reading labs off of PDFs. I do think they’ve raised some interesting modeling and data-generation avenues to explore, as time allows.</p>


</section>

 ]]></description>
  <guid>https://noamf.ink/blog/posts/pdf-lab-reports/</guid>
  <pubDate>Thu, 15 Feb 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Isolated Python Environments for Quarto Blogs</title>
  <link>https://noamf.ink/blog/posts/quarto-isolated-python-envs/</link>
  <description><![CDATA[ 





<p>I recently moved this blog over to <a href="https://quarto.org/">Quarto</a>, mostly to make it easier to integrate relevant pieces of code into a few upcoming posts. Quarto files integrate markdown with executable code. The resulting documents are a bit like Jupyter notebooks in the sense that they interleave code and commentary. Unlike Jupyter notebooks, they can be edited in any text editor, which is an important feature for me as a long time Vim user. Quarto also has native support for blogging and better support for compilation to multiple formats. So far, I’ve had a good experience.</p>
<p>The one issue I’ve come across has to do with Quarto’s handling of Python environments. Quarto provides some documentation about using it alongside <a href="https://quarto.org/docs/projects/virtual-environments.html">virtual environments</a>, but the documentation seems to assume one virtual environment per project. For the purposes of a blog, I’d like one virtual environment per post; otherwise changing the environment can break rendering of an earlier post.</p>
<p>Helpfully, Quarto documents support <a href="https://quarto.org/docs/computations/python.html#kernel-selection">specifying a Jupyter kernel</a> to use for rendering. Without a bit more structure, though, this option can lead to loose ends. Kernels dependencies can be altered iteratively, which can make them very difficult to reproduce. Even if they are created programmatically from environment specifications, manually recreating environments and kernels can be tedious.</p>
<p>In Quarto blogs, each post lives in <a href="https://quarto.org/docs/websites/website-blog.html#posts-directory">its own directory</a> For my purposes, I wanted it to be clear exactly what’s needed for each post’s code. I also wanted to be able to trivially reconstruct all the relevant kernels from their corresponding <code>requirements.txt</code> files. This is primarily because I believe in the principle of reproducibility and want others to be able to run the code easily. A secondary reason is that I edit this blog on two computers, and don’t want to have to worry about keeping environments up-to-date between them.</p>
<p>The result I landed on has two steps. The first is two include a <code>requirements.txt</code> file in the directory of each post that has executable python code. This puts the environment spec close to the relevant code, and makes it clear how to construct an environment that can run the code in that specific post. On its own, though, this leaves the problem of having to manually register and update kernels with Jupyter.</p>
<p>This leads to the second step, which is a bash script, copied below. The script searches for <code>requirements.txt</code> files in the posts directory, automatically creates a corresponding venv from each one, and registers that venv as a Jupyter kernel under the same name as the blog post’s directory. This reduces the setup for isolated python environments to running a single script. Now, we just need to specify the Jupyter kernel for the post, as linked above, in each post.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Install jupyter into the current enviornment if not already present</span></span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install jupyter</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create directory for virtualenvs</span></span>
<span id="cb1-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mkdir</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${HOME}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">/venvs"</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># For each post that has a requirements.txt file, create the corresponding kernel</span></span>
<span id="cb1-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">find</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"quarto-site/blog/posts"</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-type</span> f <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-name</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"requirements.txt"</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">|</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">read</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">requirements_file</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">do</span></span>
<span id="cb1-9"></span>
<span id="cb1-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the name of the post corresponding to this requirements.txt file</span></span>
<span id="cb1-11">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">dir_path</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$(</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dirname</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${requirements_file}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">)</span></span>
<span id="cb1-12">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">dir_name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$(</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">basename</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${dir_path}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">)</span></span>
<span id="cb1-13"></span>
<span id="cb1-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create, activate, and install requirements for the venv</span></span>
<span id="cb1-15">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">venv_path</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${HOME}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">/venvs/</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${dir_name}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb1-16">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">python</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> venv <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${venv_path}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--clear</span> </span>
<span id="cb1-17">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">source</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${venv_path}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">/bin/activate"</span></span>
<span id="cb1-18">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-r</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${requirements_file}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb1-19">    </span>
<span id="cb1-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Register the venv as a Jupyter kernel</span></span>
<span id="cb1-21">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install ipykernel</span>
<span id="cb1-22">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">python</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> ipykernel install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--user</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--name</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${dir_name}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--display-name</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${dir_name}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb1-23">    </span>
<span id="cb1-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Deactivate the venv</span></span>
<span id="cb1-25">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">deactivate</span></span>
<span id="cb1-26"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">done</span></span></code></pre></div>



 ]]></description>
  <guid>https://noamf.ink/blog/posts/quarto-isolated-python-envs/</guid>
  <pubDate>Tue, 13 Feb 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Datasets for Document AI</title>
  <link>https://noamf.ink/blog/posts/pdf-parsing-datasets/</link>
  <description><![CDATA[ 





<p>This post will aim to provide an overview of real and synthetic datasets available for training Document AI models, with a focus on PDF documents. This is the second post in a <a href="../../../blog/posts/pdf-series-intro">series</a> on document parsing. The <a href="../../../blog/posts/pdf-parsing-tasks">previous post</a> explored a number of datasets used in benchmarking tasks. This post will focus on datasets used to train foundation models, which requires more data than can exist in annotated datasets used in benchmarking.</p>
<section id="pdf-datasets" class="level3">
<h3 class="anchored" data-anchor-id="pdf-datasets">PDF datasets</h3>
<p>There are a lot of easily-accessible PDFs in the world, so the main benefit of these datasets is that someone else has gone through the effort of collecting them in one place. Some PDF datasets are grouped by type.</p>
<p>PDFs can be either scanned or native. Native PDFs contain digital information about characters and other visual markings. Scanned PDFs can be thought of as images, in the sense that they encode pixel-level data, but no higher-level information about the contents of the document. Native PDFs can easily be converted into “scanned” PDFs by applying visual filters that replicate artifacts from the scanning process. <a href="https://github.com/sparkfish/augraphy">Augraphy</a> is a great tool for this.</p>
<p>This idea is illustrated by the <a href="https://github.com/sparkfish/shabby-pages">shabby pages</a> dataset, which contains about ~6,000 native documents that have been artificially altered to simulate scanning or faxing. The advantage of this kind of manipulation is that the ground truth is available. The manipulations were done with Augraphy, linked above.</p>
<p>The <a href="https://github.com/machine-intelligence-laboratory/DDI-100">DDI-100</a> dataset began with ~6,500 real document pages, and constructed various altered versions to arrive at ~100,000 page images.</p>
<p>The <a href="https://www.nist.gov/srd/nist-special-database-2">SFRS</a> dataset has ~5,500 document images of tax forms from 1988. “The document images in this database appear to be real forms prepared by individuals, but the images have been automatically derived and synthesized using a computer.” In other words, the real original filled-in values seem to have been overwritten by synthetically generated values. This dataset can be thought of as a synthetic <a href="../../../blog/posts/pdf-parsing-tasks#forms-parsing">form parsing</a> dataset. I haven’t been able to find a direct download link; I’ll update here when I hear back from the NIST “scientific contact” for the database.</p>
<p>The <a href="https://github.com/tpn/pdfs/">pdfs</a> repository on github is a collection of ~1,700 “technically-oriented” PDFs. They seem to be mostly native PDFs, which could similarly be digitally altered.</p>
<p>The <a href="https://data.nist.gov/od/id/mds2-2531">IIT CDIP</a> dataset has a large number of legal documents from the state of Illinois’ “lawsuits against tobacco companies in the 1990s”. The website says there are ~7,000,000 documents, but other sources (such as the <a href="https://arxiv.org/pdf/2111.15664.pdf">DONUT paper</a>) put the number at ~11,000,000.</p>
<p>The <a href="https://www.kaggle.com/search?q=PDF+data+datasetFileTypes%3Apdf+datasetSize%3Amedium+datasetSize%3Alarge">kaggle dataset collection</a> has a number of PDF datasets, most have in the hundreds of files. Note that the preceding link filters to datasets that have at least one PDF, but that PDF is sometimes a user agreement or a data dictionary. There is no way to filter to datasets that contain only PDF files.</p>
<p>The <a href="https://huggingface.co/datasets?sort=downloads&amp;search=PDF">hugging face dataset collection</a> allows for search by dataset name but not dataset type.</p>
<p>Finally, the <a href="https://paperswithcode.com/datasets?q=PDF+Document&amp;v=lst&amp;o=match">papers with code</a> dataset list has similar functionality.</p>
<p>All three of those dataset collections seem to have relevant datasets, though they are mostly on the smaller side. Unlike the other two, papers with code does not host the datasets it lists.</p>
<section id="synthetic-pdf-pipelines" class="level4">
<h4 class="anchored" data-anchor-id="synthetic-pdf-pipelines">Synthetic PDF pipelines</h4>
<p>As mentioned above, <a href="https://github.com/sparkfish/augraphy">Augraphy</a> can synthetically generate “scanned” or otherwise distorted PDFs from existing PDFs.</p>
<p><a href="https://github.com/clovaai/donut/tree/master/synthdog">SynthDoG</a> was developed as part of the <a href="https://arxiv.org/pdf/2111.15664.pdf">OCR-free Document Understanding Transformer</a> paper. It produces synthetic documents by taking the following steps:</p>
<ul>
<li>randomly select background images from imagenet</li>
<li>randomly select “paper texture” from photos of paper</li>
<li>apply random distortion to paper texture</li>
<li>generate a random grid-based layout of text boxes of differing sizes and locations</li>
<li>randomly select font, style, and size of text</li>
<li>fill the text boxes with text from wikipedia</li>
<li>post-process the image</li>
</ul>
<p>See appendix A.2 of the linked paper for more details. This seems like a neat pipeline.</p>
<p>SynthDoG seems to be a more advanced version of <a href="https://github.com/clovaai/synthtiger">synthtiger</a>, which takes many of the same steps.</p>
</section>
</section>
<section id="ideas-for-generating-pdf-documents-for-medical-data-extraction" class="level3">
<h3 class="anchored" data-anchor-id="ideas-for-generating-pdf-documents-for-medical-data-extraction">Ideas for generating PDF documents for medical data extraction</h3>
<p>At this point, I’m thinking about extracting medical data from PDFs as a mix of two kinds of tasks. The first is <a href="../../../blog/posts/pdf-parsing-tasksd#forms-parsing">form parsing</a>, which will be relevant when the PDF has distinct entry fields for specific pieces of information. This is often the case with demographic information, lab test results, vital signs, prescriptions, etc. The second is closer to <a href="../../../blog/posts/pdf-parsing-tasks#document-question-answering">question answering</a>, which will be relevant when the information is contained in free-text clinical notes. This often happens when we’re interested in medical histories, the specifics of a procedure or radiology diagnosis, or patient symptoms.</p>
<p>Regardless of how or whence we extract the information, the task is variation of Key Information Extraction (KIE). For each potential FHIR field, we would like to know if the field is present in the document, and if so what its value is.</p>
<p>Generating realistic medical PDFs will involve creating randomly generated plausible layouts for structured medical information of the type that can be generated by <a href="https://github.com/synthetichealth/synthea">Synthea</a>, and using LLMs to to generate plausible clinical notes that encode known pieces of information. Combining these elements with the synthetic data generation pipelines linked above, I think it might be possible to train a Document AI model for medical KIE.</p>
</section>
<section id="next-steps" class="level3">
<h3 class="anchored" data-anchor-id="next-steps">Next steps</h3>
<p>In the next post, we’ll take a look at evaluating some pre-trained OCR and Document AI models on medical information extraction tasks. Depending on the results, we might explore creating realistic synthetic medical documents for improved medical KIE training.</p>


</section>

 ]]></description>
  <guid>https://noamf.ink/blog/posts/pdf-parsing-datasets/</guid>
  <pubDate>Fri, 09 Feb 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Benchmarking Tasks for Document Analysis</title>
  <link>https://noamf.ink/blog/posts/pdf-parsing-tasks/</link>
  <description><![CDATA[ 





<p>This is the first post in a <a href="../../../blog/posts/pdf-series-intro">series</a> on document parsing. I’m starting with a review of the benchmarking tasks because I want to understand which problems the field is orienting itself around, and how they relate to the challenges of <a href="../../../blog/posts/pdf-series-intro#challenges-in-extracting-medical-data-from-pdfs">extracting data from medical Documents</a>.</p>
<p>The <a href="https://huggingface.co/blog/document-ai">Accelerating Document AI</a> post by Hugging Face covers some of the same ground. It was helpful context in writing this post and is worth a read.</p>
<p>I’ll update this post as I come across new relevant tasks.</p>
<section id="document-question-answering" class="level3">
<h3 class="anchored" data-anchor-id="document-question-answering">Document Question Answering</h3>
<p>The document question answer task takes a document (PDF, image, or text) and a natural language question about the document as inputs, and produces a natural language answer to that question as an output.</p>
<p>The <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD2.0</a> dataset is a dataset in which the documents are text (extracted from wikipedia articles), the questions are English, and each question is paired with one or more related substrings of the corresponding document, or a special token indicating that the question is not answered in the document. There are ~500 documents, with ~100,000 question-answer pairs, and an additional ~50,000 adversarial questions that seem like they might be answered in the document but are not.</p>
<p>These documents are not PDFs, but might be used to train the language part of a PDF parsing model. In the worst case, they can be automatically converted to either scanned or digital PDF format with simulated visual artifacts.</p>
<p>The <a href="https://www.docvqa.org/">DocVQA</a> collection has 4 distinct data sets. The <a href="https://www.docvqa.org/datasets/docvqa">DocVQA</a> dataset has ~12,000 images with ~50,000 questions. Answers are also text extracted from the given document image. There are also similar, smaller datasets for Infographics (rather than documents) and handwritten documents. The final dataset groups documents into a <a href="https://arxiv.org/abs/2104.14336">collection</a>, and any document in the collection can be used to answer the question.</p>
<p>The datasets can be downloaded <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=downloads">here</a>, after creation of an account.</p>
<p>The related <a href="https://benchmarks.elsa-ai.eu/?ch=2&amp;com=downloads">PFL-DocVQA</a> dataset, which has ~1,000,000 question-answer pairs on ~110,000 documents in total. At the moment, it seems only ~250,000 are public, but the remainder should be published soon.</p>
<p>Both <a href="https://rrc.cvc.uab.es" class="uri">https://rrc.cvc.uab.es</a> and <a href="https://benchmarks.elsa-ai.eu" class="uri">https://benchmarks.elsa-ai.eu</a>, which host the DocVQA and PFL-DocVQA datasets respectively also have other interesting image-to-text datasets that might be useful for pre-training parts of a document model.</p>
<p>The <a href="https://rrc.cvc.uab.es/?ch=23&amp;com=introduction">DUDE</a> data set contains ~5,000 PDF documents, with ~20,000 question-answer pairs. Questions can be <em>extractive</em>, in which case the answer must appear in the document, as for the datasets above, or <em>abstractive</em>, in which case the answer does not appear in the document.</p>
<p>The <a href="https://github.com/adlnlp/pdfvqa#document-image-and-layout-structure-pickle-files">PDF-VQA</a> is also a visual question answer dataset. The authors point to the multi-page documents as the main differentiator from DocVQA. It contains ~12,000 single-page documents, and ~1,100 multi-page documents. It also contains ~140,000 question-answer pairs, and information about the logical and spatial relation of words in the document. The “relational graphs” that encode these relationships are described in section 3.2 of the the <a href="https://arxiv.org/pdf/2304.06447.pdf">accompanying paper</a>.</p>
</section>
<section id="document-classification" class="level3">
<h3 class="anchored" data-anchor-id="document-classification">Document Classification</h3>
<p>Document classification is what it sounds like.</p>
<p>The <a href="https://paperswithcode.com/dataset/rvl-cdip">RVL-CDIP</a> dataset contains ~400,000 images of documents, each of which is assigned to one of 16 classes. The leaderboard is <a href="https://paperswithcode.com/sota/document-image-classification-on-rvl-cdip">here</a>, and can be downloaded <a href="https://adamharley.com/rvl-cdip/">here</a>.</p>
<p>The <a href="https://www.kaggle.com/datasets/patrickaudriaz/tobacco3482jpg">Tobacco3482</a> dataset contains a ~3,492 images classified into 10 categories.</p>
</section>
<section id="document-layout-analysis" class="level3">
<h3 class="anchored" data-anchor-id="document-layout-analysis">Document Layout Analysis</h3>
<p>The layout analysis task aims to segment the documents into blocks, and classify each block. Common block labels are things like <em>text</em>, <em>title</em>, <em>figure</em>, <em>table</em>, etc.</p>
<p><a href="https://github.com/ibm-aur-nlp/PubLayNet#getting-data">PubLayNet</a> contains ~360,000 scientific papers drawn from PubMed. The dataset was constructed by “automatically matching XML representations and the contents” of the papers.</p>
</section>
<section id="document-parsing" class="level3">
<h3 class="anchored" data-anchor-id="document-parsing">Document Parsing</h3>
<p>The document parsing task aims to extract specific kinds of information from a document. In the case of forms, the goal is to extract key-value pairs, where each key is the name of a form-field, and the value is the user-entered text for that field. This is also sometimes called Key Information Extraction (KIE). Form and table parsing may be especially relevant to pulling information from medical records.</p>
<section id="forms-parsing" class="level4">
<h4 class="anchored" data-anchor-id="forms-parsing">Forms Parsing</h4>
<p>The Form Understanding from Noisy Scanned Documents (<a href="https://guillaumejaume.github.io/FUNSD/download/">FUNDS</a>) dataset has ~200 forms, with information about words and their locations, <em>semantic entities</em> — collections of related words —, and relations links between key-value pairs. <a href="https://guillaumejaume.github.io/FUNSD/description/">Here</a> is a nice summary-by-example of the data available on each form.</p>
<p>The related <a href="https://github.com/doc-analysis/XFUND">XFUND</a> dataset extends this idea to documents in other languages. The dataset repository doesn’t have much information, but it’s described in the <a href="https://arxiv.org/abs/2104.08836v3">LayoutXLM</a> paper. XFUND contains ~1,400 annotated forms, at ~200 per language. The dataset contains standard OCR data (i.e.&nbsp;words with bounding boxes), as well as semantically significant sets of words, each of which is called a <em>semantic entity</em>. It also contains annotated key-value pairs; a major goal is identifying the value associated with a given key.</p>
</section>
<section id="table-parsing" class="level4">
<h4 class="anchored" data-anchor-id="table-parsing">Table parsing</h4>
<p>The <a href="https://huggingface.co/datasets/bsmock/pubtables-1m">PubTables-1M</a> dataset contains ~575,000 pages, with a total of ~950,000 fully annotated tables. <a href="https://user-images.githubusercontent.com/10793386/139559159-cd23c972-8731-48ed-91df-f3f27e9f4d79.jpg">Table annotations</a> include column headers and <em>projected row headers</em>, which are used to subdivide the table into vertical sections (see the linked image for an example). The dataset can also be used to learn to identify tables in documents.</p>
<p>The <a href="https://developer.ibm.com/exchanges/data/all/fintabnet/">FinTabNet</a> dataset “contains complex tables from the annual reports of S&amp;P 500 companies with detailed table structure annotations”. It has ~90,000 pages with ~110,000 tables. The annotations here seem to just indicate a bounding box for each <em>table</em>, and a bounding box for each <em>cell</em> in the table. See the bottom of <a href="https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/f57cf3f6-e972-48ff-ab7b-3771ba7b9683/view?access_token=317644327d84f5d75b4782f97499146c78d029651a7c7ace050f4a7656033c30">this notebook</a> for examples. There is also an improved version called <a href="https://huggingface.co/datasets/bsmock/FinTabNet.c">FinTabNet.c</a> available on hugging face.</p>
<p>On first look, it doesn’t seem that either dataset includes a class for the kinds of notes that often appear in lab test reports (<a href="https://www.researchgate.net/profile/John-Flach/publication/267494297/figure/fig1/AS:392037380706311@1470480403318/An-example-of-a-typical-format-used-to-report-results-of-blood-analysis-to-the-physician.png">example 1</a>, <a href="https://i.pinimg.com/736x/8e/a6/ef/8ea6efe0d12a1a580e8d1b3390a3e066.jpg">example 2</a>). These kinds of notes often disrupt table recognition, as the same column headers apply above and below the note. I’ve had trouble in the past where table identifiers either considered the parts of a table separated by a note to be distinct tables, or else only recognized the part of the table that precedes the note, as the remaining parts do not have identifiable column headers.</p>
<p>Because the FinTabNet does not contain semantic information about table structure, models trained on it might be better able to recognize the notes as distinct cells; this will be interesting to explore.</p>
<p>There is also the <a href="https://huggingface.co/datasets/bsmock/ICDAR-2013-Table-Competition-Corrected">ICDAR2013 table competition dataset</a>, which is available in corrected form on hugging face. The dataset is described in <a href="https://corpora.tika.apache.org/base/docs/bug_trackers/tabula/tabula-java-LINK-49-1.pdf">this paper</a>. It contains table location and structure information.</p>
<p>A summary of these three datasets, as well as interesting commenatry about using them together, can be found <a href="https://arxiv.org/pdf/2303.00716.pdf">here</a>.</p>
</section>
<section id="receipts-parsing" class="level4">
<h4 class="anchored" data-anchor-id="receipts-parsing">Receipts Parsing</h4>
<p>The <a href="https://rrc.cvc.uab.es/?ch=13&amp;com=introduction">SROIE dataset</a> has ~1,000 receipt images. Each receipt is annotated with standard word / bounding box information, as well as a number of key-value pairs (the example keys given are <em>company</em>, <em>date</em>, <em>address</em>, and <em>total</em>).</p>
<p>The <a href="https://github.com/clovaai/cord">CORD</a> dataset has ~1,0000 receipts. Each receipt is annotated with word / bounding box information, as well as line-level and receipt-level classifications.</p>
</section>
<section id="datasets-in-other-languages" class="level4">
<h4 class="anchored" data-anchor-id="datasets-in-other-languages">Datasets in other languages</h4>
<p>I’ll focus on English datasets for now, both because I can read English and because the documents I’m interested in working with are in English. There are interesting datasets in other languages, especially in Chinese. In particular, the <a href="https://rrc.cvc.uab.es/?ch=21">HUSTL-CELL</a> dataset looks relevant to key information extraction, and is primarily in Chinese.</p>
</section>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>This post is meant to provide an overview of the kinds of tasks that are currently used to evaluate methods for Document AI. The corresponding datasets can be used for training or fine-tuning as well.</p>
<p>My overall impression is that there is low availability of large, high-quality annotated datasets, especially for the parsing-based tasks. This seems to be a bottleneck for training, though perhaps less so for evaluation.</p>
<p>In the next post in the series, we’ll look at unlabeled datasets that can be used for self-supervised pre-training. I will be especially interested in methods for generating synthetic labeled data. For medical document information extraction, it may be relevant to see if plausible documents of varied structures can be created using synthetic FHIR data from <a href="https://github.com/synthetichealth/synthea">Synthea</a>.</p>


</section>

 ]]></description>
  <guid>https://noamf.ink/blog/posts/pdf-parsing-tasks/</guid>
  <pubDate>Wed, 07 Feb 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>A Series of posts about PDF Parsing with AI</title>
  <link>https://noamf.ink/blog/posts/pdf-series-intro/</link>
  <description><![CDATA[ 





<p>This is an introductory post for a series on AI for analyzing PDFs. The posts in the series are listed below.</p>
<ul>
<li><a href="../../../blog/posts/pdf-parsing-tasks">Tasks</a> — On established benchmarking tasks in the literature</li>
<li><a href="../../../blog/posts/pdf-parsing-datasets">Datasets</a> — On available labeled and unlabeld PDF datasets</li>
<li><a href="../../../blog/posts/pdf-lab-reports">Table Parsing for Bloodwork</a> — A first look at using existing <a href="../../../blog/posts/pdf-parsing-tasks#table-parsing">table parsing</a> tools on standard lab reports</li>
</ul>
<p>This list will be updated as the posts are written.</p>
<p>I’m not an expert in document modeling, and I might make obvious or silly mistakes along the way. If you spot any, or have other questions or comments, please <a href="mailto:noamf.ink@proton.me">let me know</a>!</p>
<p>I’m writing this series as a way of documenting and organizing my preliminary explorations of this field; I hope it will useful to others as well.</p>
<section id="motivation-for-catching-up-on-state-of-the-art-ai-for-pdfs" class="level3">
<h3 class="anchored" data-anchor-id="motivation-for-catching-up-on-state-of-the-art-ai-for-pdfs">Motivation for catching up on state-of-the-art AI for PDFs</h3>
<p>There’s a lot of medical information that only exists in PDF documents. Much of this information is old, from the days before the <a href="https://www.hhs.gov/hipaa/for-professionals/special-topics/hitech-act-enforcement-interim-final-rule/index.html">HITECH</a> act kicked off the EHR revolution. While most newer clinical information is stored digitally <em>somewhere</em>, patients and even healthcare providers can often access it only in PDF form.</p>
<p>I’ve long been interested in pulling medical data out of scanned PDF documents. Part of this is a personal interest — I have a stack of physical documents representing decades of my family’s medical histories.</p>
<p>But I also have a professional interest in this problem. In nearly a decade of working on EHR-based precision medicine, I’ve seen many situations in which critically important information is only accessible in PDF form.</p>
<p>Historically, extracting data from PDFs — especially scanned PDFs — has been tedious and error-prone. The biggest reason for this is that the PDF format is a visual rather than semantic encoding, and the spatial elements of PDF forms are rarely consistent between hospitals or clinics, or even within a single clinic over time.</p>
<p>It’s been about two years since I looked seriously at the state-of-the-art in PDF parsing, and a lot has changed in the world of AI in that time. I’m curious at how much closer we are to solving this problem in the era of massive open-source models.</p>
</section>
<section id="challenges-in-extracting-medical-data-from-pdfs" class="level3">
<h3 class="anchored" data-anchor-id="challenges-in-extracting-medical-data-from-pdfs">Challenges in extracting medical data from PDFs</h3>
<p>The most general formulation of the task I’m interested is this:</p>
<blockquote class="blockquote">
<p><em>Given a digital or scanned PDF, produce a structured <a href="https://www.hl7.org/fhir/documentation.html">FHIR</a> document that contains all clinical information present in the original PDF, including clinical notes text.</em></p>
</blockquote>
<p>The FHIR format is the industry standard format for storing and transfering clinical information.</p>
<p>There are a few challenges involved in this task that may be somewhat non-standard. It’s not necessary to solve all of these challenges for automated PDF parsing to be useful, but it will be interesting to see whether and how they’re addressed in existing models and benchmarking tasks. A few of these challenges are listed below.</p>
<ul>
<li>We don’t know what information will be present in any given PDF.</li>
<li>Each PDF may contain print and handwritten text.</li>
<li>Some PDFs contain information on multiple procedures or tests across many distinct dates.</li>
<li>The same medical concept may have many different names.</li>
</ul>
<p>There are also simpler tasks that might still be useful, such as extracting specific demographic or clinical information, or all clinical notes, from a document if available.</p>
<p>In the past, I’ve had some success running scanned documents through OCR, and then writing rules-based engines to pull vitals and labs values on the basis of spatial orientation and a few other factors.</p>
<p>It’s possible to write these rules in a relatively robust way, but a rules-based approach will always be fragile. First, medical forms are notorious for the variety of their spatial patterns. Second, because the OCR engine is not connected to any medical knowledge, it doesn’t know to correct for character- or token-level mistakes that are obvious in context. Making these corrections post-hoc is possible, but it adds substantial engineering and computational complexity. I’m especially curious about whether and how newer methods are integrating the linguistic and clinical context into the character recognition process.</p>
<p>My hope is that it will be possible to build on modern approaches and open-source models for PDF parsing to avoid rules-based systems, and to expand the kinds of information it’s possible to extract beyond just simple labs and vitals.</p>
</section>
<section id="order-of-posts-in-the-series" class="level3">
<h3 class="anchored" data-anchor-id="order-of-posts-in-the-series">Order of posts in the series</h3>
<p>In my experience, machine-learning methods often optimize for recongized and established benchmarking tasks in their sub-field. Understanding these tasks in detail from the start can help get sense for how the field is orienting itself. The next post in the series will be about relevant benchmarking tasks.</p>
<p>Next, I’ll take a look at available datasets, as well as the feasibility of generating realistic data for training purposes. I find this is also helpful to do before reading about specific models, as it provides context for which objectives models are trained against, and when it’s possible to use self-supervised objectives.</p>
<p>Finally, I’ll take a closer look at existing model architectures and open-source models. This part of the series may expand into several posts about testing out, fine-tuning or altering existing models.</p>


</section>

 ]]></description>
  <guid>https://noamf.ink/blog/posts/pdf-series-intro/</guid>
  <pubDate>Tue, 06 Feb 2024 05:00:00 GMT</pubDate>
</item>
</channel>
</rss>
