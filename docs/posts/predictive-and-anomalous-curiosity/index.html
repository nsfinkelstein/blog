<!DOCTYPE html>
<html lang="en">

<head>
  
    <link rel="alternate" type="application/atom+xml" title="atom" href="https://noamf.ink/atom.xml">
  
  <meta charset="utf-8">
  <title>Noam's site</title>
</head>

<body>
  <section class="section">
    <div class="container">
      
<h3>
  Types of curiosity: predictive and anomalous
</h3>
2023-03-20
<p>A couple days ago I wrote about an idea for <a href="https://noamf.ink/posts/intrinsic-reinforcement-learning/">curiosity in reinforcement
learning</a>. Since then I was reminded that
there's already a class of methods in reinforcement learning that go by the same name, summarized in
<a href="https://arxiv.org/pdf/1808.04355.pdf">this paper</a>.</p>
<p>These methods use a different idea to generate &quot;curiosity.&quot; They allow an agent to make predictions
about the effects of its actions. The more incorrect the agent is about its predictions, the more
it's penalized. It has an incentive to learn more about the kinds of situations that leave it least
about to predict the future.</p>
<p>The idea I wrote about was to simulate the agent's &quot;mental model&quot; of the world by using a model
trained using an unsupervised approach. The agent might contain, in part, an auto-encoder element
that encodes the full state of the environment or distinct objects in the environment. When the
agent comes across input that the auto-encoder encodes poorly, it gets a poor reward, indicating
that input is unlike what the agent has seen so far.</p>
<p>I think human curiosity involves elements of both of these paradigms (and probably many others).
It's true that we're constantly making predictions about what we think will happen as a result of
our actions, and that when those predictions are wrong, that's an opportunity for learning. We can
get curious about why we were wrong, and try to figure out how to be less wrong in the future.</p>
<p>But curiosity doesn't require this kind of prediction. When we encounter something that's totally
new to us — something we don't quite know how to make sense of — we can get curious
about it, even without making predictions. This kind of curiosity is better simulated by an
unsupervised loss, which reflects how well a model captures patterns in the data. Just as we're
driven to find out more about <em>anomalies</em> in our lives, the agent would be driven to refine its
&quot;mental model&quot; to account for the input that strikes it as strange, i.e. that does not match the
patterns it has identified. I think of this as a more <em>reflective</em> kind of curiosity. As we try to
synthesize and assimilate information about the world, we notice and wonder about pieces that don't
quite fit in.</p>
<p>What would happen if we created an agent with both predictive and anomalous curiosity? I wonder.</p>


    </div>
  </section>
  <img src="//counter.websiteout.net/compte.php?S=noamfinkelstein&C=5&D=0&N=0&M=1" alt="" border="0" height="0px" width="0px" />
</body>

</html>
