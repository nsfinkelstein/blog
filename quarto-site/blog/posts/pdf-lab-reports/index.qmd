---
title: "Trying out Table Parsing Models on Labwork Reports"
date: "2024-02-15"
categories: []
description: " "
jupyter: pdf-lab-reports
---

```{python}
#| eval: true
#| echo: false
#| output: false

# create PNGs for the TATR models
from labs import create_tatr_pngs, create_tatr_structure_pngs
create_tatr_pngs()
create_tatr_structure_pngs()
```


This post is part of a [series](/blog/posts/pdf-series-intro) about Document AI, specifically with
an eye towards parsing medical documents. In this post, we'll take a quick look at how a few 
[table parsing](/blog/posts/pdf-parsing-tasks#table-parsing) tools do when applied to lab test 
reports.

### Structure of lab reports

Below are three sample reports from three different labs. 

![](report-images/accesa-000.png){.lightbox width=200px group="unmarked"}
![](report-images/labcorp-002.png){.lightbox width=200px group="unmarked"}
![](report-images/quest-000.png){.lightbox width=200px group="unmarked"}

The reports are basically tables, but they have two interesting features that will make them
difficult to parse. 

First, the tables sometimes include sub-headers, which specify the name of the lab panel that the
test falls under. This is in evidence in the third report, but it's a bit of an unusual case because
each panel has only one test. In some cases, some but not all tests are associated with panels.

Second, the tables contain comments. In the second document, the comments actually sometimes contain
the result of the lab test. In the third document, the comments apply to the lab test procedure
itself, not to the result for an individual patient. In both cases, the comments warp the structure
of the table.

As far as I know, there isn't a great open dataset of lab reports, primarily because the include
private health information. That said, a number of people have uploaded their own reports to the
internet, and I used many of them in these checks. I didn't include them in the post due to privacy
considerations. Running table parsing models on those reports didn't reveal anything unexpected, but
they did show that the models have a much harder time with noisy, scanned documents.

### Table Transformer Model

The open-source 
[Table Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer)
is trained on detecting tables and identifying their structure. The model and training data are
described in [this paper](https://arxiv.org/abs/2110.00061). The same base model is trained on two
separate tasks, described below.

#### Table detection

The first task is to identify tables in a document.

![](tatr-report-images/accesa-000.png){.lightbox width=200px group="tatr"}
![](tatr-report-images/labcorp-002.png){.lightbox width=200px group="tatr"}
![](tatr-report-images/quest-000.png){.lightbox width=200px group="tatr"}

In two cases, the model has a hard time distinguishing between structured information at the top of
the document, and the structured lab results. In the second document, the model seems to be tricked
by the space in the table introduced by the longest comment. It may also be relevant that the
records below that table have more spacing between them than those above, as they each have their
own (short) comment. It's also interesting that the model has high confidence for the third
document, but relatively low confidence for the first two.

#### Table structure identification

To run the structure identification model, we first pull the highest-confidence table identified by
the detection model above. The structure identification model is very sensitive to the amount of
padding around the table it receives as input. This is a noted issue with the pre-trained model, and
can likely be resolved by fine-tuning with an augmented dataset. For the purposes of using the
pre-trained model, I had to add padding around the tables identified above to get reasonable
predictions. It's possible that adding more or less padding would result in better predictions, but
because that would be a superficial solution to the problem, I didn't take the time to find out.

Running this model produces the following results.

![](tatr-structure-report-images/accesa-000.png){.lightbox width=200px group="tatr-segmented"}
![](tatr-structure-report-images/labcorp-002.png){.lightbox width=200px group="tatr-segmented"}
![](tatr-structure-report-images/quest-000.png){.lightbox width=200px group="tatr-segmented"}

The model does a relatively good job with the first document, though not as good as I would have
expected given how structured the table is. In the second document, it seems to be exactly correct
for the part of the table identified by the table detection model, but has no change on the rest of
the table (I did not separately extract the rest of the table to see how it would have done). In the
third document, the model doesn't recognize the rows corresponding to the antibody tests. This might
have something to do with the fact the test names extend past what it has identified as the relevant
column. The size of the column seems to have to do with the front-matter, rather than the lab tests.

### Commercial options

There are a few commercial options for table parsing. The easiest to try out was Nanonets, which
produced the outputs below.

![](nanonets-report-images/accesa-000.png){.lightbox width=200px group="nanonets"}
![](nanonets-report-images/labcorp-002.png){.lightbox width=200px group="nanonets"}
![](nanonets-report-images/quest-000.png){.lightbox width=200px group="nanonets"}

The first striking part of this result is that none of the table cells overlap. I don't know if this
is due to post-processing or because of how they trained their model, but it's nice. Their structure
detection seems to be better, by and large, but also less flexible, as there is no notion of a
spanning cell or a header row. 

The first document is pretty much exactly right. In the second document, they miss the last lab
test, and also separate the `REFERENCE INTERVAL` column into two columns, which would cause problems
downstream. In the third document, they also miss the last lab, but otherwise do well. Because
they're missing a concept for a spanning cell, in the last two documents the comments are being
sliced up. This might be addressable in post-processing.

### Next steps

The two models both had difficulty with the kinds of things I expected they might have difficulty
with, discussed above. Additionally, the open source model had difficulty distinguishing between the
lab test results and structured text at the top of the document, which I did not anticipate.

Any training or fine-tuning to get around these kinds of problems would have to be based on
synthetic data, because there is not enough open lab-report data out there.

Here are a few considerations for generating this kind of data set:

- Reports should have structured text at the top that looks like it _might_ be part of the lab
  result table but is not, to help the model distinguish between form fields and lab results.
- The lab results table should include header row columns for lab tests, with representation of the
  cases in which all, some, or no labs have associated panels, and a good distribution over the
  number of tests per panel.
- The lab results rows should sometimes span multiple lines, with overflow wrapping at both the
  column and row level (this caused issues in lab reports not shown in this post).
- The lab results table should have both inline and stand-alone comments.
- Ideally the lab names, reference ranges, values, units, etc. used in the generated document should
  all be plausible. Depending on how the model is trained, this might help with text recognition.

I don't think it would be difficult to create a dataset with these properties that also contains
enough variety to train for every lab report I've come across. Because there are lots of little labs
out there, and their reports all have different formats, it would be prudent to add even more
structural variety than that, even though that will likely increase the required model size and the
training burden.

One other thought is that the approach of these table parsing models seems somewhat indirect. They
first visually identify the table and its structure, and then run OCR on each cell. I wonder whether
there is a more direct approach, where the input is the lab report document, and the output is one
or more CSV files containing the results.

Overall, I don't think general purpose table parsing tools will be effective for reading labs off
of PDFs. I do think they've raised some interesting modeling and data-generation avenues to explore,
as time allows.
