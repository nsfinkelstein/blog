{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Benchmarking Tasks for Document Analysis\"\n",
        "date: \"2024-02-07\"\n",
        "date-modified: \"2024-02-09\"\n",
        "categories: []\n",
        "description: \" \"\n",
        "draft: true\n",
        "---"
      ],
      "id": "d1b3e76c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "from pathlib import Path\n",
        "python_path = Path(__file__).parent / 'venv' / 'bin' / 'path'"
      ],
      "id": "9f4ceaa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Overview\n",
        "\n",
        "### Table parsing\n",
        "\n",
        "Microsoft has a \"table transformer\" that they trained on each of the two table parsing datasets, and\n",
        "then also on the combination of the two.\n",
        "\n",
        "<https://github.com/microsoft/table-transformer#news>\n",
        "\n",
        "Try these both on a lab test report. In particular, see how they account for \"notes\".\n",
        "\n",
        "![](report-images/accesa-000.png){.lightbox width=100px group=\"unmarked\"}\n",
        "![](report-images/labcorp-000.png){.lightbox width=100px group=\"unmarked\"}\n",
        "![](report-images/labcorp-001.png){.lightbox width=100px group=\"unmarked\"}\n",
        "![](report-images/labcorp-002.png){.lightbox width=100px group=\"unmarked\"}\n",
        "![](report-images/meditech-000.png){.lightbox width=100px group=\"unmarked\"}\n",
        "![](report-images/quest-000.png){.lightbox width=100px group=\"unmarked\"}\n",
        "![](report-images/quest-001.png){.lightbox width=100px group=\"unmarked\"}\n",
        "\n",
        "##### Tasks\n",
        "\n",
        "![](nanonets-report-images/accesa-000.png){.lightbox width=100px group=\"nanonets\"}\n",
        "![](nanonets-report-images/labcorp-000.png){.lightbox width=100px group=\"nanonets\"}\n",
        "![](nanonets-report-images/labcorp-001.png){.lightbox width=100px group=\"nanonets\"}\n",
        "![](nanonets-report-images/labcorp-002.png){.lightbox width=100px group=\"nanonets\"}\n",
        "![](nanonets-report-images/meditech-000.png){.lightbox width=100px group=\"nanonets\"}\n",
        "![](nanonets-report-images/quest-000.png){.lightbox width=100px group=\"nanonets\"}\n",
        "![](nanonets-report-images/quest-001.png){.lightbox width=100px group=\"nanonets\"}\n",
        "\n",
        "\n",
        "\n",
        "Questio\n",
        "\n",
        "Model Taxonomy & Specific Models\n",
        "\n",
        "###### DocLLM: <https://arxiv.org/abs/2401.00908>\n",
        "\n",
        "Takes OCR outputs as inputs. Operates on words, blocks and bounding boxes. Attention heads attend to\n",
        "spatial and textual context.\n",
        " abc\n",
        "\n",
        "###### Rationale Distillation: <https://arxiv.org/abs/2311.09612>\n",
        "\n",
        "\n",
        "Datasets\n",
        "<!---->\n",
        "<!-- ```{python} -->\n",
        "<!-- import labs -->\n",
        "<!-- labs.test() -->\n",
        "<!-- ``` -->"
      ],
      "id": "a76ba38d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}