---
title: "A Series of posts about PDF Parsing with AI"
date: "2024-02-06"
date-modified: "2024-02-08"
categories: []
description: " "
---

This is an introductory post for a series on AI for analyzing PDFs. The posts in the series are 
listed below. 

- [Tasks](/blog/posts/pdf-parsing-tasks) &mdash; On established benchmarking tasks in the
literature
- [Datasets](/blog/posts/pdf-parsing-datasets) &mdash; On available labeled and unlabeld
PDF datasets
- Models &mdash; On published model architectures and weights

This list will be updated as the posts are written.

I'm not an expert in document modeling, and I might make obvious or silly mistakes along the way. If
you spot any, or have other questions or comments, please 
<a href="mailto:noamf.ink@proton.me">let me know</a>! 

I'm writing this series as a way of documenting and organizing my preliminary explorations of this
field; I hope it will useful to others as well.

### Motivation for catching up on state-of-the-art AI for PDFs

There's a lot of medical information that only exists in PDF documents. Much of this information is
old, from the days before the 
[HITECH](https://www.hhs.gov/hipaa/for-professionals/special-topics/hitech-act-enforcement-interim-final-rule/index.html)
act kicked off the EHR revolution. While most newer clinical information is stored digitally
_somewhere_, patients and even healthcare providers can often access it only in PDF form.

I've long been interested in pulling medical data out of scanned PDF documents. Part of this is a 
personal interest &mdash; I have a stack of physical documents representing decades of my family's
medical histories. 

But I also have a professional interest in this problem. In nearly a decade of working on EHR-based
precision medicine, I've seen many situations in which critically important information is only
accessible in PDF form.

Historically, extracting data from PDFs &mdash; especially scanned PDFs &mdash; has been tedious and
error-prone. The biggest reason for this is that the PDF format is a visual rather than semantic
encoding, and the spatial elements of PDF forms are rarely consistent between hospitals or clinics,
or even within a single clinic over time.

It's been about two years since I looked seriously at the state-of-the-art in PDF parsing, and a lot
has changed in the world of AI in that time. I'm curious at how much closer we are to solving this
problem in the era of massive open-source models.

### Challenges in extracting medical data from PDFs

The most general formulation of the task I'm interested is this:

> _Given a digital or scanned PDF, produce a structured
[FHIR](https://www.hl7.org/fhir/documentation.html) document that contains all clinical information
present in the original PDF, including clinical notes text._

The FHIR format is the industry standard format for storing and transfering clinical information.

There are a few challenges involved in this task that may be somewhat non-standard. It's not
necessary to solve all of these challenges for automated PDF parsing to be useful, but it will be
interesting to see whether and how they're addressed in existing models and benchmarking tasks. A
few of these challenges are listed below.

- We don't know what information will be present in any given PDF.
- Each PDF may contain print and handwritten text.
- Some PDFs contain information on multiple procedures or tests across many distinct dates.
- The same medical concept may have many different names.

There are also simpler tasks that might still be useful, such as extracting specific demographic
or clinical information, or all clinical notes, from a document if available.

In the past, I've had some success running scanned documents through OCR, and then writing
rules-based engines to pull vitals and labs values on the basis of spatial orientation and a few
other factors. 

It's possible to write these rules in a relatively robust way, but a rules-based approach will
always be fragile. First, medical forms are notorious for the variety of their spatial patterns.
Second, because the OCR engine is not connected to any medical knowledge, it doesn't know to correct
for character- or token-level mistakes that are obvious in context. Making these corrections
post-hoc is possible, but it adds substantial engineering and computational complexity. I'm
especially curious about whether and how newer methods are integrating the linguistic and clinical
context into the character recognition process.

My hope is that it will be possible to build on modern approaches and open-source models for PDF
parsing to avoid rules-based systems, and to expand the kinds of information it's possible to
extract beyond just simple labs and vitals. 

### Order of posts in the series

In my experience, machine-learning methods often optimize for recongized and established
benchmarking tasks in their sub-field. Understanding these tasks in detail from the start can help
get sense for how the field is orienting itself. The next post in the series will be about relevant
benchmarking tasks.

Next, I'll take a look at available datasets, as well as the feasibility of generating realistic
data for training purposes. I find this is also helpful to do before reading about specific models,
as it provides context for which objectives models are trained against, and when it's possible to
use self-supervised objectives.

Finally, I'll take a closer look at existing model architectures and open-source models. This part
of the series may expand into several posts about testing out, fine-tuning or altering existing
models.
