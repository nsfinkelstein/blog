{
  "hash": "466dcbc1ba1893c829ddf64376b471eb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Trying out Table Parsing Models on Labwork Reports\ndate: '2024-02-15'\ncategories: []\ndescription: ' '\n---\n\n\n\nThis post is part of a [series](/blog/posts/pdf-series-intro) about Document AI, specifically with\nan eye towards parsing medical documents. In this post, we'll take a quick look at how a few \n[table parsing](/blog/posts/pdf-parsing-tasks#table-parsing) tools do when applied to lab test \nreports.\n\n### Structure of lab reports\n\nBelow are three sample reports from three different labs. \n\n![](report-images/accesa-000.png){.lightbox width=100px group=\"unmarked\"}\n![](report-images/labcorp-002.png){.lightbox width=100px group=\"unmarked\"}\n![](report-images/quest-000.png){.lightbox width=100px group=\"unmarked\"}\n\nThe reports are basically tables, but they have a two interesting features that will make them\ndifficult to parse. \n\nFirst, the tables sometimes include sub-headers, which specify the name of the lab panel that the\ntest falls under. This is in evidence in the third report, but it's a bit of an unusual case because\neach panel has only one test. In some cases, some but not all tests are associated with panels.\n\nSecond, the tables contain comments. In the second document, the comments actually sometimes contain\nthe result of the lab test. In the third document, in the comments apply to the lab test procedure\nitself, not to the result for an individual patient. In both cases, the comments warp the structure\nof the table.\n\nAs far as I know, there isn't a great open dataset of lab reports, primarily because the include\nprivate health information. That said, a number of people have uploaded their own reports to the\ninternet, and I used many of them in these checks. I didn't include them in the post due to privacy\nconsiderations. Running table parsing models on those reports didn't reveal anything unexpected, but\nthey did show that the models have a much harder time with noisy, scanned documents.\n\n### Table Transformer Model\n\nThe open-source \n[Table Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer)\nis trained on detecting tables and identifying their structure. The model and training data are\ndescribed in [this paper](https://arxiv.org/abs/2110.00061). The same base model is trained on two\nseparate tasks, described below.\n\n#### Table detection\n\nThe first task is to identify tables in a document.\n\n![](tatr-report-images/accesa-000.png){.lightbox width=100px group=\"tatr\"}\n![](tatr-report-images/labcorp-002.png){.lightbox width=100px group=\"tatr\"}\n![](tatr-report-images/quest-000.png){.lightbox width=100px group=\"tatr\"}\n\nIn a two cases, the model has a hard time distinguishing between structured information at the top\nof the document, and the structured lab results. In the second document, the model seems to be\ntricked by the space in the table introduced by the longest comment. It may also be relevant that\nthe records below that table have more spacing between them than those above, as they each have\ntheir own (short) comment. It's also interesting that the model has high confidence for the third\ndocument, but relatively low confidence for the first two.\n\n#### Table structure identification\n\nTo run the structure identification model, we first pull the highest-confidence table identified by\nthe detection model above. The structure identification model is very sensitive to the amount of\npadding around the table it receives as input. This is a noted issue with the pre-trained model, and\ncan likely be resolved by fine-tuning with an augmented dataset. For the purposes of using the\npre-trained model, I had to add padding around the tables identified above to get reasonable\npredictions. It's possible that adding more or less padding would result in better predictions, but\nbecause that would be a superficial solution to the problem, I didn't take the time to find out.\n\nRunning this model produces the following results.\n\n![](tatr-structure-report-images/accesa-000.png){.lightbox width=100px group=\"tatr-segmented\"}\n![](tatr-structure-report-images/labcorp-002.png){.lightbox width=100px group=\"tatr-segmented\"}\n![](tatr-structure-report-images/quest-000.png){.lightbox width=100px group=\"tatr-segmented\"}\n\nThe model does a relatively good job with the first document, though not as good as I would have\nexpected given how structured the table is. In the second document, it seems to be exactly correct\nfor the part of the table identified by the table detection model, but has no change on the rest of\nthe table (I did not separately extract the rest of the table to see how it would have done). In the\nthird document, the model doesn't recognize the rows corresponding to the antibody tests. This might\nhave something to do with the fact the test names extend past what it has identified as the relevant\ncolumn. The size of the column seems to have to do with the front-matter, rather than the lab tests.\n\n### Commercial options\n\nThere are a few commercial options for table parsing. The easiest to try out was Nanonets, which\nproduced the outputs below.\n\n![](nanonets-report-images/accesa-000.png){.lightbox width=100px group=\"nanonets\"}\n![](nanonets-report-images/labcorp-002.png){.lightbox width=100px group=\"nanonets\"}\n![](nanonets-report-images/quest-000.png){.lightbox width=100px group=\"nanonets\"}\n\nThe first striking part of this result is that none of the table cells overlap. I don't know if this\nis due to post-processing or because of how they trained their model, but it's nice. Their structure\ndetection seems to be better, by and large, but also less flexible, as there is no notion of a\nspanning cell or a header row. \n\nThe first document is pretty much exactly right. In the second document, they miss the last lab\ntest, and also separate the `REFERENCE INTERVAL` column into two columns, which would cause problems\ndownstream. In the third document, they also miss the last lab, but otherwise do will. Because\nthey're missing a concept for a spanning cell, in the last two documents the comments are being\nsliced up. This might be addressable in post-processing.\n\n### Next steps\n\nThe two models both had difficulty with the kinds of things I expected they might have difficulty\nwith, discussed above. Additionally, the open source model had difficulty distinguishing between the\nlab test results and structured text at the top of the document, which I did not anticipate.\n\nAny training or fine-tuning to get around these kinds of problems would have to be based on\nsynthetic data, because there is not enough open lab-report data out there.\n\nHere are a few considerations for generating this kind of data set:\n\n- Reports should have structured text at the top that looks like it _might_ be part of the lab\nresult table but is not, to help the model distinguish between form fields and lab results.\n- The lab results table should include header row columns for lab tests, with representation of the\n    cases in which all, some, or no labs have associated panels, and a good distribution over the\n    number of tests per panel.\n- The lab results rows should sometimes span multiple lines, with overflow wrapping at both the\ncolumn and row level (this caused issues in lab reports not shown in this post).\n- The lab results table should have both inline and stand-alone comments.\n- Ideally the lab names, reference ranges, values, units, etc. used in the generated document should\n  all be plausible. Depending on how the model is trained, this might help with text recognition.\n\nI don't think it would be difficult to create a dataset with these properties that also contains\nenough variety to train for every lab report I've come across. Because there are lots of little labs\nout there, and their reports all have different formats, it would be prudent to add even more\nstructural variety than that, even though that will likely increase the required model size and the\ntraining burden.\n\nOne other thought is that the approach of these table parsing models seems somewhat indirect. They\nfirst visually identify the table and its structure, and then run OCR on each cell. I wonder whether\nthere is a more direct approach, where the input is the lab report document, and the output is one\nor more CSV files containing the results.\n\nOverall, I don't think general purposes table parsing tools will be effective for reading labs off\nof PDFs. I do think they've raised some interesting modeling and data-generation avenues to explore,\nas time allows.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}