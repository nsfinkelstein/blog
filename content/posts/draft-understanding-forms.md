+++
title = "Revisiting the octupus thought experiment"
draft = true
+++

Emily Bender and XXX wrote a thoughtful paper a few years ago about the limits of large language
model's abilities. It contained a prominant thought experiment about an octupuse that I found quite
compelling at the time. Looking back, I find it a bit less compelling. Part of the thought
experiment depended on the octupus' inability to reason about physical objects in the real world.
But as we're increasingly seeing, that ability can be generated by reading enough about the basic
rules of physics. It's true the octopus would not have that knowledge from lived experience, but it
could still have it.

Yann LeCun makes a similar point in a recent presentation:

> LLMs have no knowledge of the underlying reality They have no common sense & they canâ€™t plan their
> answer.

https://docs.google.com/document/d/1fBOIOos3xQWglVF07VCG8O8Rm8klyj5byXLMl00HDjs/edit
