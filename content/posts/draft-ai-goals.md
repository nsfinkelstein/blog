+++
title = "Can artificial intelligences have goals?"
draft = true
+++

In a recent interview, [Kelsey Piper](https://www.vox.com/authors/kelsey-piper) of Vox made the
following comment about goals:

> we don’t know if the A.I.s have goals. And some people will say they clearly don’t have goals
> because how would something like that have goals? And I’m just kind of like, I don’t know.
> Evolution was just selecting repeatedly on ability to have babies, and here we are. We have goals.
> Why does that process get you things that have goals? I don’t know. They just showed up along the
> way. Large language models are just selecting repeatedly on token prediction and then
> reinforcement. Does that have goals? I think if anybody tells you yes or no, they are overstating
> what we know and what we can know.

Thinking about whether artificial intelligences can develop goals in the context of evolution is
interesting.

I'm not an expert in evolution, but I do think it's easy to see why having goals is useful from an
evolutionary perspective. The implicit objective that evolutionary pressures respond to is survival
to reproduction. Survival in the real world is a difficult, open-ended problem. Creatures that are
able to break that problem down into smaller pieces, each of which eventually contribute to the
larger objective, and each of which they feel an inclination to achieve, are more likely to survive.
So it doesn't seem mysterious that the process of evolution produces being with goals. Those goals
look different across species, but we all have them to some degree, I think generally the complexity
of the goal increases with the size of the species.

Does AI, trained on next-token prediction, have anything remotely like that? What if we add
reinforcement of different kinds?
